{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "5UNsY-2Cpu4f"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "\n",
        "# =============================\n",
        "# (i)  Gridworld Wrapper\n",
        "# =============================\n",
        "class GridworldEnv:\n",
        "    def __init__(self, grid_size=(4, 4), start=(0, 0), goal=(3, 3), slip_prob=0.05):\n",
        "        self.grid_size = grid_size\n",
        "        self.start = start\n",
        "        self.goal = goal\n",
        "        self.state = start\n",
        "        self.actions = ['up', 'down', 'left', 'right']\n",
        "        self.action_dict = {\n",
        "            'up': (-1, 0),\n",
        "            'down': (1, 0),\n",
        "            'left': (0, -1),\n",
        "            'right': (0, 1)\n",
        "        }\n",
        "        self.slip_prob = slip_prob\n",
        "\n",
        "    def reset(self):\n",
        "        self.state = self.start\n",
        "        return self.state\n",
        "\n",
        "    def get_transition_distribution(self, state, action):\n",
        "        \"\"\"\n",
        "        Returns a list of dictionaries representing the probability distribution over outcomes.\n",
        "        Each outcome dict contains:\n",
        "          - 'next_state': the state after the transition\n",
        "          - 'reward': the immediate reward received\n",
        "          - 'done': whether this outcome terminates the episode\n",
        "          - 'probability': the probability of this outcome\n",
        "\n",
        "        The dynamics are:\n",
        "          - With probability (1 - slip_prob): the agent moves as intended.\n",
        "          - With probability slip_prob: the agent stays in the same state.\n",
        "        \"\"\"\n",
        "        if action not in self.actions:\n",
        "            raise ValueError(\"Invalid action!\")\n",
        "\n",
        "        # Compute intended next state\n",
        "        movement = self.action_dict[action]\n",
        "        intended_state = (state[0] + movement[0], state[1] + movement[1])\n",
        "        # Clamp to grid boundaries\n",
        "        intended_state = (max(0, min(intended_state[0], self.grid_size[0] - 1)),\n",
        "                          max(0, min(intended_state[1], self.grid_size[1] - 1)))\n",
        "\n",
        "        # Outcome 1: intended move (with probability 1 - slip_prob)\n",
        "        if intended_state == self.goal:\n",
        "            reward_intended = 0\n",
        "            done_intended = True\n",
        "        else:\n",
        "            reward_intended = -1\n",
        "            done_intended = False\n",
        "        outcome_intended = {\n",
        "            'next_state': intended_state,\n",
        "            'reward': reward_intended,\n",
        "            'done': done_intended,\n",
        "            'probability': 1 - self.slip_prob\n",
        "        }\n",
        "\n",
        "        # Outcome 2: slip and remain in the same state (with probability slip_prob)\n",
        "        if state == self.goal:\n",
        "            reward_slip = 0\n",
        "            done_slip = True\n",
        "        else:\n",
        "            reward_slip = -1\n",
        "            done_slip = False\n",
        "        outcome_slip = {\n",
        "            'next_state': state,\n",
        "            'reward': reward_slip,\n",
        "            'done': done_slip,\n",
        "            'probability': self.slip_prob\n",
        "        }\n",
        "\n",
        "        return [outcome_intended, outcome_slip]\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "    def step_sample(self, action):\n",
        "        \"\"\"\n",
        "        Samples one outcome from the transition distribution for the given action.\n",
        "        This method updates the environment's state and returns a tuple:\n",
        "          (next_state, reward, done)\n",
        "        \"\"\"\n",
        "        outcomes = self.get_transition_distribution(self.state, action)\n",
        "        probs = [outcome['probability'] for outcome in outcomes]\n",
        "        chosen_index = np.random.choice(len(outcomes), p=probs)\n",
        "        chosen_outcome = outcomes[chosen_index]\n",
        "        self.state = chosen_outcome['next_state']\n",
        "        return chosen_outcome['next_state'], chosen_outcome['reward'], chosen_outcome['done']\n",
        "\n",
        "\n",
        "    def render(self):\n",
        "        grid = np.full(self.grid_size, '.')\n",
        "        grid[self.goal] = 'G'\n",
        "        grid[self.state] = 'S'\n",
        "        for row in grid:\n",
        "            print(\" \".join(row))\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "\n",
        "def main():\n",
        "    # Create the environment instance\n",
        "    env = GridworldEnv()\n",
        "\n",
        "    # 1. Reset the environment\n",
        "    print(\"Resetting environment:\")\n",
        "    initial_state = env.reset()\n",
        "    print(\"Initial state:\", initial_state)\n",
        "\n",
        "    # 2. Get the transition distribution for a specific action from the initial state\n",
        "    action = 'right'\n",
        "    print(f\"\\nTransition distribution for action '{action}' from state {initial_state}:\")\n",
        "    transitions = env.get_transition_distribution(initial_state, action)\n",
        "    for outcome in transitions:\n",
        "        print(outcome)\n",
        "\n",
        "\n",
        "    # 3. Using the step_sample function (samples one outcome and updates the state)\n",
        "    print(f\"\\nUsing step_sample() for action '{action}':\")\n",
        "    next_state, reward, done = env.step_sample(action)\n",
        "    print(\"Sampled outcome -> Next state:\", next_state, \"Reward:\", reward, \"Done:\", done)\n",
        "\n",
        "    # 4. Render the current state of the gridworld\n",
        "    print(\"\\nRendering the gridworld:\")\n",
        "    env.render()\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    main()\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "7Tu-XDb1qrid",
        "outputId": "2f4fc040-aa64-4d7a-8817-f9545dbc2253"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Resetting environment:\n",
            "Initial state: (0, 0)\n",
            "\n",
            "Transition distribution for action 'right' from state (0, 0):\n",
            "{'next_state': (0, 1), 'reward': -1, 'done': False, 'probability': 0.95}\n",
            "{'next_state': (0, 0), 'reward': -1, 'done': False, 'probability': 0.05}\n",
            "\n",
            "Using step_sample() for action 'right':\n",
            "Sampled outcome -> Next state: (0, 1) Reward: -1 Done: False\n",
            "\n",
            "Rendering the gridworld:\n",
            ". S . .\n",
            ". . . .\n",
            ". . . .\n",
            ". . . G\n"
          ]
        }
      ]
    }
  ]
}